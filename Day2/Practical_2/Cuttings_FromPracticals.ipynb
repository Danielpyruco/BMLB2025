{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f454a-e789-4a2d-8d61-afb8fa47a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shortpractical1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc85989-a8a0-409c-8e0c-9b11c7b049e2",
   "metadata": {},
   "source": [
    "## Other optimisation algorithms\n",
    "\n",
    "Gradient descent is one way to get the minimum of a function, but it is certainly not the only one. You might have noticed that we ran it for a thousand iterations above and it moved slowly. This is because the gradients become very small when the predictions are almost correct (if your hypothesis predicts 0.001 but it should be 0 exactly, you can see how that would lead to exceedingly small steps to correct this). Therefore, we might wish to turn to other optimisation methods. <br> There are many other ones that incorporate all sorts of smart tricks. Those of you who've followed the BiBC Essentials Course should remember some of them! Here, you're going to optimise logistic regression not using simple gradient descent, but one of the more involved optimisers. We won't go into how they work (that's a numerical mathematics course in its own right), just know that they are faster and will require inputs in a slightly different form. You need to:\n",
    "\n",
    "* Skim [this](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html) and try to understand what the first 4 arguments mean.\n",
    "* You'll need to supply this advanced optimisation algorithm with 1: the function to optimise, and 2. (optionally, but we will do it) its derivative(s).\n",
    "* To do that:\n",
    "    1. Make a copy of the gradient descent function (named `linAlgGradient`), with the following arguments: `linAlgGradient(thetas, x, y, method = 'logistic')`. Notice how the arguments have been reordered. This is because `fmin_bfgs` wants the first argument of each function to be its parameters (thetas).<br> This copy should **not return the new thetas** (as in our normal gradient descent function) **but return the array of gradients** (partial derivatives) of the cost function. Hence this function doesn't need an alpha parameter. <br> Make sure to wrap the returned values in `np.ravel()` so they are in the dimension fmin_bfgs wants (so something like  `return np.ravel(partialDerivativesThetas)`).\n",
    "    2. The function to optimise is the cost function. Make a copy of it that has the thetas as its first arguments `costFuncLogReg2(thetas, x, y)`. Other than that it stays exactly the same.\n",
    "* Once that's done, run the code cell below it to see the resulting thetas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb1ab7-df84-4001-92a3-85ac259cb7a4",
   "metadata": {},
   "source": [
    "Note that the warnings are because certain values, when exponentiated, become np.nan (too large or too small to hold in a floating point value) and because log(0) is -Inf. In the end, these values are ignored due to `np.nansum` and the -Inf values are also not tallied. Hence, nothing to worry about. Sometimes people add very small sums to the np.log() calls so the cost never becomes -infinity. With gradient descent it would have taken ages to find them, with BFGS it's rather quick. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
